{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421ca2e0-dab6-43de-998c-c3bf7947f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4993 images belonging to 24 classes.\n",
      "Found 615 images belonging to 24 classes.\n",
      "Found 3110 images belonging to 24 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PIYUSH LAHORI\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PIYUSH LAHORI\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 541ms/step - accuracy: 0.1005 - loss: 2.9690 - val_accuracy: 0.3203 - val_loss: 2.0741\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 213ms/step - accuracy: 0.3204 - loss: 1.9657 - val_accuracy: 0.5317 - val_loss: 1.3638\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 206ms/step - accuracy: 0.4816 - loss: 1.4204 - val_accuracy: 0.6537 - val_loss: 1.1217\n",
      "Epoch 4/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 238ms/step - accuracy: 0.6014 - loss: 1.1100 - val_accuracy: 0.6569 - val_loss: 0.9875\n",
      "Epoch 5/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 210ms/step - accuracy: 0.6820 - loss: 0.8979 - val_accuracy: 0.8211 - val_loss: 0.6869\n",
      "Epoch 6/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 198ms/step - accuracy: 0.7283 - loss: 0.7718 - val_accuracy: 0.7854 - val_loss: 0.6889\n",
      "Epoch 7/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 193ms/step - accuracy: 0.7422 - loss: 0.7332 - val_accuracy: 0.8293 - val_loss: 0.5632\n",
      "Epoch 8/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 199ms/step - accuracy: 0.7705 - loss: 0.6618 - val_accuracy: 0.8146 - val_loss: 0.5901\n",
      "Epoch 9/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 192ms/step - accuracy: 0.7869 - loss: 0.6133 - val_accuracy: 0.8618 - val_loss: 0.5374\n",
      "Epoch 10/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 191ms/step - accuracy: 0.7941 - loss: 0.6053 - val_accuracy: 0.8228 - val_loss: 0.5084\n",
      "Epoch 11/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 193ms/step - accuracy: 0.8422 - loss: 0.4860 - val_accuracy: 0.8325 - val_loss: 0.4428\n",
      "Epoch 12/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 193ms/step - accuracy: 0.8267 - loss: 0.4916 - val_accuracy: 0.8341 - val_loss: 0.6110\n",
      "Epoch 13/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 200ms/step - accuracy: 0.8450 - loss: 0.4430 - val_accuracy: 0.8878 - val_loss: 0.3715\n",
      "Epoch 14/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 199ms/step - accuracy: 0.8663 - loss: 0.4306 - val_accuracy: 0.9122 - val_loss: 0.2808\n",
      "Epoch 15/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 190ms/step - accuracy: 0.8620 - loss: 0.4136 - val_accuracy: 0.8488 - val_loss: 0.5742\n",
      "Epoch 16/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 189ms/step - accuracy: 0.8441 - loss: 0.4404 - val_accuracy: 0.8797 - val_loss: 0.4635\n",
      "Epoch 17/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 190ms/step - accuracy: 0.8492 - loss: 0.4665 - val_accuracy: 0.8732 - val_loss: 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Define Paths\n",
    "data_dir = \"C:/Users/PIYUSH LAHORI/Downloads/dataset/fruits-360_dataset_original-size/fruits-360-original-size\"  # Replace with the path to the Fruit360 dataset\n",
    "train_dir = os.path.join(data_dir, \"Training\")\n",
    "val_dir = os.path.join(data_dir, \"Validation\")\n",
    "test_dir = os.path.join(data_dir, \"Test\")\n",
    "\n",
    "# Step 2: Data Generators for Training and Testing\n",
    "batch_size = 32\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "\n",
    "# Data generator for training and validation with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    validation_split=0.2,  # Keep validation split\n",
    "    rotation_range=20,  # Randomly rotate images\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically\n",
    "    shear_range=0.2,  # Apply shear transformations\n",
    "    zoom_range=0.2,  # Randomly zoom in on images\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    fill_mode=\"nearest\",  # Fill pixels after transformations\n",
    ")\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "val_data = train_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "# Data generator for testing\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,  # Use batch size 1 for real-time testing\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Step 3: Define and Train the Model with Regularization and Dropout\n",
    "num_classes = len(train_data.class_indices)\n",
    "class_indices = train_data.class_indices  # Save the mapping of class names to indices\n",
    "class_labels = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            16, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),  # Add dropout to reduce overfitting\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3),\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(0.001),  # Add L2 regularization\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),  # Higher dropout in fully connected layers\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"fruit360_model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e218b1e1-ef53-4206-97c3-e04d8b3c2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PIYUSH~1\\AppData\\Local\\Temp\\tmp96f_1679\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PIYUSH~1\\AppData\\Local\\Temp\\tmp96f_1679\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\PIYUSH~1\\AppData\\Local\\Temp\\tmp96f_1679'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 24), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2257396972816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257396972048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257396971856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257396972432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257396972624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257396971664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257399022224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257399023184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257399022416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2257399024528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open(\"fruit360_model3.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580f8759-000b-4654-8b16-3d48ed2e2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit the real-time detection.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Real-Time Camera Detection\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess the captured frame to feed into the model.\"\"\"\n",
    "    image = cv2.resize(image, (img_width, img_height))  # Resize to model input size\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = image.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Load TFLite model for inference\n",
    "interpreter = tf.lite.Interpreter(model_path=\"fruit360_model3.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Open the camera\n",
    "cap = cv2.VideoCapture(0)  # Change to 1 if using an external camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open the camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'q' to quit the real-time detection.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame for model input\n",
    "    input_image = preprocess_image(frame)\n",
    "\n",
    "    # Perform inference\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_image)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    predicted_class = np.argmax(output_data)\n",
    "    confidence = np.max(output_data)\n",
    "\n",
    "    # Display the prediction on the video feed\n",
    "    predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        f\"Fruit: {predicted_label} ({confidence*100:.2f}%)\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Fruit Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80cc491-e2d4-446c-8b02-3d2567a87879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   7/3110\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 19ms/step - accuracy: 0.1561 - loss: 2.2611     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PIYUSH LAHORI\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3110/3110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 16ms/step - accuracy: 0.9363 - loss: 0.2284\n",
      "Test Accuracy: 96.82%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make predictions on a few test samples\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):  \u001b[38;5;66;03m# Test on first 5 samples\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     test_image, test_label \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m      8\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_image)\n\u001b[0;32m      9\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on a few test samples\n",
    "for i in range(5):  # Test on first 5 samples\n",
    "    test_image, test_label = test_data.next()\n",
    "    prediction = model.predict(test_image)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    actual_class = np.argmax(test_label)\n",
    "    print(f\"Predicted: {class_labels[predicted_class]}, Actual: {class_labels[actual_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d2daf-9418-4a49-8bfc-d2468f635b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
